{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Summarization.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyONE+qrDeyLMPfrPpMpIClg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jos-k19/TextSummarization/blob/main/ExtractiveImageSumm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extractive Summarization"
      ],
      "metadata": {
        "id": "Us9N1LZNO2Os"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auTz-LvCeZIT",
        "outputId": "b2e9bbac-7dc7-4969-d048-663253eaf26b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "qR0JLku10Pcd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnnLnMURNBIh"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import sent_tokenize as nlkt_sent_tokenize\n",
        "from nltk.tokenize import word_tokenize as nlkt_word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "n4a1hosv0LqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculates cosine similarity\n",
        "def similarity(v1, v2):\n",
        "    score = 0.0\n",
        "    if np.count_nonzero(v1) != 0 and np.count_nonzero(v2) != 0:\n",
        "        score = ((1 - cosine(v1, v2)) + 1) / 2\n",
        "    return score\n",
        "\n",
        "def sent_tokenize(text):\n",
        "    sents = nlkt_sent_tokenize(text)\n",
        "    sents_filtered = []\n",
        "    for s in sents:\n",
        "        sents_filtered.append(s)\n",
        "    return sents_filtered\n",
        "\n",
        "def cleanup_sentences(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    sentences = sent_tokenize(text)\n",
        "    sentences_cleaned = []\n",
        "    for sent in sentences:\n",
        "        words = nlkt_word_tokenize(sent)\n",
        "        words = [w for w in words if w not in string.punctuation]\n",
        "        words = [w for w in words if not w.lower() in stop_words]\n",
        "        words = [w.lower() for w in words]\n",
        "        sentences_cleaned.append(\" \".join(words))\n",
        "    return sentences_cleaned\n",
        "\n",
        "def get_tf_idf(sentences):\n",
        "    vectorizer = CountVectorizer()\n",
        "    sent_word_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "    transformer = TfidfTransformer(norm=None, sublinear_tf=False, smooth_idf=False)\n",
        "    tfidf = transformer.fit_transform(sent_word_matrix)\n",
        "    tfidf = tfidf.toarray()\n",
        "\n",
        "    centroid_vector = tfidf.sum(0)\n",
        "    centroid_vector = np.divide(centroid_vector, centroid_vector.max())\n",
        "\n",
        "    feature_names = vectorizer.get_feature_names()\n",
        "\n",
        "    relevant_vector_indices = np.where(centroid_vector > 0.3)[0]\n",
        "\n",
        "    word_list = list(np.array(feature_names)[relevant_vector_indices])\n",
        "    return word_list\n"
      ],
      "metadata": {
        "id": "6Hi18RwaNOHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Populate word vector with all embeddings.\n",
        "#This word vector is a look up table that is used\n",
        "#for getting the centroid and sentences embedding representation.\n",
        "def word_vectors_cache(sentences, embedding_model):\n",
        "    word_vectors = dict()\n",
        "    for sent in sentences:\n",
        "        words = nlkt_word_tokenize(sent)\n",
        "        for w in words:\n",
        "            word_vectors.update({w: embedding_model.wv[w]})\n",
        "    return word_vectors"
      ],
      "metadata": {
        "id": "eXRd64HU1hPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Words Embedding"
      ],
      "metadata": {
        "id": "oG7U-3ce0uHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Sentence embedding representation with sum of word vectors\n",
        "def build_embedding_representation(words, word_vectors, embedding_model):\n",
        "    embedding_representation = np.zeros(embedding_model.vector_size, dtype=\"float32\")\n",
        "    word_vectors_keys = set(word_vectors.keys())\n",
        "    count = 0\n",
        "    for w in words:\n",
        "        if w in word_vectors_keys:\n",
        "            embedding_representation = embedding_representation + word_vectors[w]\n",
        "            count += 1\n",
        "    # print(embedding_representation)\n",
        "\n",
        "    if count != 0:\n",
        "       embedding_representation = np.divide(embedding_representation, count)\n",
        "\n",
        "    return embedding_representation\n",
        "\n"
      ],
      "metadata": {
        "id": "T5jRnRBMNSpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Score Sentences"
      ],
      "metadata": {
        "id": "2zRsP48T14aT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize(text, emdedding_model):\n",
        "    raw_sentences = sent_tokenize(text)\n",
        "    clean_sentences = cleanup_sentences(text)\n",
        "    # for i, s in enumerate(raw_sentences):\n",
        "    #     print(i, s)\n",
        "    # for i, s in enumerate(clean_sentences):\n",
        "    #     print(i, s)\n",
        "    centroid_words = get_tf_idf(clean_sentences)\n",
        "    # print(len(centroid_words), centroid_words)\n",
        "    word_vectors = word_vectors_cache(clean_sentences, emdedding_model)\n",
        "    #Centroid embedding representation\n",
        "    centroid_vector = build_embedding_representation(centroid_words, word_vectors, emdedding_model)\n",
        "    # print(\"centroid_vector\", centroid_vector)\n",
        "    sentences_scores = []\n",
        "    for i in range(len(clean_sentences)):\n",
        "        scores = []\n",
        "        words = clean_sentences[i].split()\n",
        "\n",
        "        #Sentence embedding representation\n",
        "        sentence_vector = build_embedding_representation(words, word_vectors, emdedding_model)\n",
        "\n",
        "        #Cosine similarity between sentence embedding and centroid embedding\n",
        "        score = similarity(sentence_vector, centroid_vector)\n",
        "        sentences_scores.append((i, raw_sentences[i], score, sentence_vector))\n",
        "    sentence_scores_sort = sorted(sentences_scores, key=lambda el: el[2], reverse=True)\n",
        "    for s in sentence_scores_sort:\n",
        "        print(s[0], s[1], s[2])\n",
        "    count = 0\n",
        "    sentences_summary = []\n",
        "    #Handle redundancy\n",
        "    for s in sentence_scores_sort:\n",
        "        if count > 100:\n",
        "            break\n",
        "        include_flag = True\n",
        "        for ps in sentences_summary:\n",
        "            sim = similarity(s[3], ps[3])\n",
        "            if sim > 0.95:\n",
        "                include_flag = False\n",
        "        if include_flag:\n",
        "            sentences_summary.append(s)\n",
        "            count += len(s[1].split())\n",
        "\n",
        "        sentences_summary = sorted(sentences_summary, key=lambda el: el[0], reverse=False)\n",
        "    # print(sentences_summary)   \n",
        "    summary = \"\\n\".join([s[1] for s in sentences_summary])\n",
        "    # print(\"Summary:\",summary)\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "Kwh8Y43o18mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation"
      ],
      "metadata": {
        "id": "DdoD4dJN2X4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = \"\"\"In an attempt to build an AI-ready workforce, Microsoft announced Intelligent Cloud Hub\n",
        "          which has been launched to empower the next generation of students with AI-ready skills.\n",
        "         Envisioned as a three-year collaborative program, Intelligent Cloud Hub will support around 100\n",
        "          institutions with AI infrastructure, course content and curriculum, developer support,\n",
        "          development tools and give students access\n",
        "           to cloud and AI services.\n",
        "          As part of the program, the Redmond giant which wants to expand its reach and is\n",
        "          planning to build a strong developer ecosystem in India with the program will set up the\n",
        "          core AI infrastructure and IoT Hub for the selected campuses.\n",
        "          The company will provide AI development tools and Azure AI services such as\n",
        "          Microsoft Cognitive Services, Bot Services and Azure Machine Learning.\n",
        "          According to Manish Prakash, Country General Manager-PS, Health and Education,\n",
        "          Microsoft India, said, \"With AI being the defining technology of our time,\n",
        "          it is transforming lives and industry and the jobs of tomorrow will\n",
        "          require a different skillset. This will require more collaborations and\n",
        "          training and working with AI. That’s why it has become more critical than ever for\n",
        "          educational institutions to integrate new cloud and AI technologies.\n",
        "          The program is an attempt to ramp up the institutional set-up and build\n",
        "          capabilities among the educators to educate the workforce of tomorrow.\"\n",
        "          The program aims to build up the cognitive skills and in-depth understanding of\n",
        "          developing intelligent cloud connected solutions for applications across industry.\n",
        "          Earlier in April this year, the company announced Microsoft Professional\n",
        "          Program In AI as a learning track open to the public.\n",
        "          The program was developed to provide job ready skills to programmers who wanted to hone their\n",
        "          skills in AI and data science with a series of online courses which featured hands-on labs and expert instructors as well.\n",
        "          This program also included developer-focused AI school that provided a bunch of assets to help build AI skills.\"\"\"\n",
        "\n",
        "clean_sentences = cleanup_sentences(text)\n",
        "words = []\n",
        "for sent in clean_sentences:\n",
        "    words.append(nlkt_word_tokenize(sent))\n",
        "model = Word2Vec(words, min_count=1, sg = 1)\n",
        "summary = summarize(text, model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pz22KyK-NVT4",
        "outputId": "65d44fb2-4af2-4f56-e67e-8bd995bd8c17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 The company will provide AI development tools and Azure AI services such as\n",
            "          Microsoft Cognitive Services, Bot Services and Azure Machine Learning. 0.8056061267852783\n",
            "1 Envisioned as a three-year collaborative program, Intelligent Cloud Hub will support around 100\n",
            "          institutions with AI infrastructure, course content and curriculum, developer support,\n",
            "          development tools and give students access\n",
            "           to cloud and AI services. 0.759765237569809\n",
            "11 This program also included developer-focused AI school that provided a bunch of assets to help build AI skills. 0.7138784676790237\n",
            "7 The program is an attempt to ramp up the institutional set-up and build\n",
            "          capabilities among the educators to educate the workforce of tomorrow.\" 0.7032933533191681\n",
            "2 As part of the program, the Redmond giant which wants to expand its reach and is\n",
            "          planning to build a strong developer ecosystem in India with the program will set up the\n",
            "          core AI infrastructure and IoT Hub for the selected campuses. 0.6782935112714767\n",
            "9 Earlier in April this year, the company announced Microsoft Professional\n",
            "          Program In AI as a learning track open to the public. 0.6742029935121536\n",
            "8 The program aims to build up the cognitive skills and in-depth understanding of\n",
            "          developing intelligent cloud connected solutions for applications across industry. 0.666585236787796\n",
            "0 In an attempt to build an AI-ready workforce, Microsoft announced Intelligent Cloud Hub\n",
            "          which has been launched to empower the next generation of students with AI-ready skills. 0.6626278012990952\n",
            "4 According to Manish Prakash, Country General Manager-PS, Health and Education,\n",
            "          Microsoft India, said, \"With AI being the defining technology of our time,\n",
            "          it is transforming lives and industry and the jobs of tomorrow will\n",
            "          require a different skillset. 0.6563727557659149\n",
            "6 That’s why it has become more critical than ever for\n",
            "          educational institutions to integrate new cloud and AI technologies. 0.6383997797966003\n",
            "5 This will require more collaborations and\n",
            "          training and working with AI. 0.6092679798603058\n",
            "10 The program was developed to provide job ready skills to programmers who wanted to hone their\n",
            "          skills in AI and data science with a series of online courses which featured hands-on labs and expert instructors as well. 0.6008436903357506\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Vy-_1Fe8X0h",
        "outputId": "e1a86603-3ee3-4b53-efde-c38fc3f51948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Envisioned as a three-year collaborative program, Intelligent Cloud Hub will support around 100\n",
            "          institutions with AI infrastructure, course content and curriculum, developer support,\n",
            "          development tools and give students access\n",
            "           to cloud and AI services.\n",
            "As part of the program, the Redmond giant which wants to expand its reach and is\n",
            "          planning to build a strong developer ecosystem in India with the program will set up the\n",
            "          core AI infrastructure and IoT Hub for the selected campuses.\n",
            "The company will provide AI development tools and Azure AI services such as\n",
            "          Microsoft Cognitive Services, Bot Services and Azure Machine Learning.\n",
            "The program is an attempt to ramp up the institutional set-up and build\n",
            "          capabilities among the educators to educate the workforce of tomorrow.\"\n",
            "This program also included developer-focused AI school that provided a bunch of assets to help build AI skills.\n"
          ]
        }
      ]
    }
  ]
}